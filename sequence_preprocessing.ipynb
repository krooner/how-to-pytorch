{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfI5CqPQ+sU+FQMWN+C7GY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krooner/how-to-pytorch/blob/main/sequence_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 언어 모델 학습에 필요한 입력 데이터 전처리\n",
        "\n",
        "> Text가 아닌 데이터를 Language Model에 투입하고 싶다. 어떻게 해야 할까?\n"
      ],
      "metadata": {
        "id": "uAfeoJs4ZFL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Import__"
      ],
      "metadata": {
        "id": "AxxrKvDDZZnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vj9WQJHrYrsl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input Data__\n",
        "- 각 Sequence는 Sentence와 같이, Whitespace로 구분되어 있는 자체 정의된 Token으로 구성\n",
        "- 예시는 일단 텍스트로 진행"
      ],
      "metadata": {
        "id": "hLPRgpZ2ZhTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "  \"Don't speak ill of others.\",\n",
        "  \"To speak ill of others is a great crime.\",\n",
        "  \"Rather rectify yourself through self-criticism.\",\n",
        "  \"In this way, if you rectify yourself, others will follow you.\",\n",
        "  \"To speak ill of others gives rise to more problems.\",\n",
        "  \"This does not do any good to society.\",\n",
        "  \"More than 80 percent people of our country live in villages.\",\n",
        "  \"Most of them are poor and illiterate.\",\n",
        "  \"Illiteracy is one of the causes of their poverty.\",\n",
        "  \"Many of the villagers are landless cultivators.\",\n",
        "  \"They cultivate the lands of other people throughout the year.\",\n",
        "  \"They get a very small portion of the crops.\",\n",
        "  \"They provide all of us with food.\",\n",
        "  \"But in want they only starve.\",\n",
        "  \"They suffer most.\",\n",
        "  \"The situation needs to be changed.\",\n",
        "  \"We live in the age of science.\",\n",
        "  \"We can see the influence of science everywhere.\",\n",
        "  \"Science is a constant companion of our life.\",\n",
        "  \"We have made the impossible things possible with the help of science.\",\n",
        "  \"Modern civilization is a contribution of science.\",\n",
        "  \"Science should be devoted to the greater welfare of mankind.\",\n",
        "  \"Rabindranath Tagore got the Nobel Prize in 1913 which is 98 years ago from today.\",\n",
        "  \"He was awarded this prize for the translation of the Bengali 'Gitanjali' into English.\",\n",
        "  \"This excellent rendering was the poet's own.\",\n",
        "  \"In the English version of Gitanjali there are 103 songs.\"\n",
        "]"
      ],
      "metadata": {
        "id": "dZzmg-1gZhe9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Order\n",
        "\n",
        "1. Sentence → List of Tokens: `torchtext.data.utils.get_tokenizer`\n",
        "2. List of List of Tokens → Vocab: `torchtext.vocab.build_vocab_from_iterator`\n",
        "3. List of Tokens → List of Indices: `torchtext.transforms.{VocabTransform, ToTensor, PadTransform, }`"
      ],
      "metadata": {
        "id": "SmmElJQxdiyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1\n",
        "---\n",
        "#### Tokenizer - using [`torchtext.data.utils.get_tokenizer`](https://pytorch.org/text/stable/data_utils.html)\n",
        "\n",
        "> Parameter\n",
        "> - tokenizer – the name of tokenizer function. __If None, it returns split() function, which splits the string sentence by space.__\n",
        "\n",
        "Text가 아닌 데이터의 경우는 Sequence로 만들기 위해서 \n",
        "1. Token에 대한 정의\n",
        "2. Token으로의 변환\n",
        "\n",
        "을 이전 단계에서 진행했을 것이기 때문에 Text Tokenizer의 기능은 필요없음.  \n",
        "(Vocab의 경우에도 `[UNK]` 즉, Unseen Token은 없다고 가정할 수 있음)\n",
        "\n",
        "### 2\n",
        "---\n",
        "\n",
        "#### Vocab - using [`torchtext.vocab.build_vocab_from_iterator`](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator)\n",
        "\n",
        "> Parameter\n",
        "> - iterator – Iterator used to build Vocab. __Must yield list or iterator of tokens.__\n",
        "\n",
        "Tokenizer로 Sentence를 List of Tokens으로 만들었으면 이걸 Iterator로 만들어서 넘김 (`yield_tokens(text)`)\n",
        "\n",
        "그리고 우리가 궁극적으로 만들 입력 데이터 형식은 다음과 같다.  \n",
        "`[CLS] Token Token Token [SEP] [PAD] [PAD] ... `\n",
        "- `[CLS]`는 Classification의 줄임말로 __Sequence의 시작을 의미__\n",
        "- `[SEP]`는 Separator의 줄임말로 __Sequence의 끝을 의미__\n",
        "- `[PAD]`는 Padding의 줄임말로 __Sequence가 `max_length`보다 짧을 때, 길이를 맞추기 위해 끼워넣는 Token을 의미__\n",
        "- `[MASK]`\n",
        "  - Masked Language Modeling (MLM) 을 위한 Token \n",
        "  - Random하게 일정 비율의 Token을 `[MASK]`로 변경한 후 Bidirectional하게 Masking된 Token이 무엇인지 맞추는 학습"
      ],
      "metadata": {
        "id": "bPYhSxQFaR05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(None)\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for t in data_iter:\n",
        "    yield tokenizer(t)\n",
        "\n",
        "# vocab = build_vocab_from_iterator(yield_tokens(text), specials=[\"[PAD]\"])\n",
        "# vocab.set_default_index(vocab[\"[PAD]\"])\n",
        "vocab = build_vocab_from_iterator(yield_tokens(text))\n",
        "vocab.insert_token('[PAD]', 0)\n",
        "vocab.insert_token('[CLS]', 1)\n",
        "vocab.insert_token('[SEP]', 2)\n",
        "vocab.insert_token('[MASK]', 3)\n",
        "\n",
        "pad_idx, cls_idx, sep_idx, msk_idx = 0, 1, 2, 3\n",
        "\n",
        "max_seq_len = max([len(vocab(tokenizer(item))) for item in text])"
      ],
      "metadata": {
        "id": "DQ5SoptDaSCj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3\n",
        "---\n",
        "#### Transforms - using [`torchtext.transforms`](https://pytorch.org/text/stable/transforms.html)\n",
        "\n",
        "`torchvision`에서 이미지 데이터에 대해 Crop, Normalize 등을 적용할 수 있는 것처럼, `torchtext`를 활용해서도 텍스트 데이터에 대해 똑같이 적용할 수 있다.\n",
        "\n",
        "- [`VocabTransform`](https://pytorch.org/text/stable/transforms.html#vocabtransform): List of Tokens → List of Indices (based on vocab)\n",
        "- [`AddToken`](https://pytorch.org/text/stable/transforms.html#addtoken): List of Indices → `[CLS]` List of Indices `[SEP]`\n",
        "- [`ToTensor`](https://pytorch.org/text/stable/transforms.html#totensor): `[CLS] ... [SEP]` → `[CLS] ... [SEP] [PAD] ...`\n",
        "  - > padding_value (Optional[int]) – Pad value to make each input in the batch of __length equal to the longest sequence in the batch.__\n",
        "  - Batch 내에서 longest sequence length에 맞게 padding을 적용함\n",
        "- [`PadTransform`](https://pytorch.org/text/stable/transforms.html#padtransform): `[CLS] ... [SEP] [PAD] ...` → 모든 시퀀스에 대해 Globally Longest Length에 맞게 Padding을 적용"
      ],
      "metadata": {
        "id": "Oj47M5aEd-qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.transforms import VocabTransform, PadTransform, ToTensor, AddToken\n",
        "\n",
        "input_ids_transform = torchtext.transforms.Sequential(\n",
        "    VocabTransform(vocab),\n",
        "    AddToken(token=cls_idx, begin=True),\n",
        "    AddToken(token=sep_idx, begin=False),\n",
        "    ToTensor(padding_value=0),\n",
        "    PadTransform(max_length=max_seq_len+2, pad_value=0)\n",
        ")\n",
        "\n",
        "def data_collate_fn(dataset_samples_list):\n",
        "    arr = [tokenizer(item) for item in dataset_samples_list]\n",
        "\n",
        "    input_ids = input_ids_transform(arr)\n",
        "    token_type_ids = torch.zeros(input_ids.size())\n",
        "    attention_mask = (input_ids != pad_idx).to(torch.int32)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}"
      ],
      "metadata": {
        "id": "ZD1J3xwqgayb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and DataLoader"
      ],
      "metadata": {
        "id": "z23FI8lxhiTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, src, tokenizer):\n",
        "      self.src = src\n",
        "      self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      src = self.src[idx]\n",
        "      return src\n",
        "\n",
        "dataset = MyDataset(text, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=1, collate_fn=data_collate_fn)"
      ],
      "metadata": {
        "id": "0rZE-9y8hk7F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0])\n",
        "for item in dataloader:\n",
        "  print(item['input_ids'])\n",
        "  print(vocab.lookup_tokens(item['input_ids'].squeeze(0)[:7].tolist()))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnVnAIExiBHI",
        "outputId": "5811c23e-1961-4570-9532-5ac7af18e140"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Don't speak ill of others.\n",
            "tensor([[  1,  36,  16,  13,   4, 104,   2,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0]])\n",
            "['[CLS]', \"Don't\", 'speak', 'ill', 'of', 'others.', '[SEP]']\n"
          ]
        }
      ]
    }
  ]
}